{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xd8zRs0PYIHz"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu iprogress ipykernel ipywidgets langchain langchain-community langchain-groq langchain-classic langchain-huggingface langchain-text-splitters pypdf python-dotenv sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import getpass\n",
        "from google.colab import files\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ecmXJKkybwX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QBxrKXPfHNn",
        "outputId": "a6a3fdb4-6e70-4fba-9eb0-3ee97421be01"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model_name=\"llama-3.1-8b-instant\",  # You can also use \"mixtral-8x7b-32768\"\n",
        ")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "print(\"Upload your CV PDF file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"Processing {filename}...\")\n",
        "\n",
        "loader = PyPDFLoader(filename)\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(f\"Success! {len(splits)} chunks created and indexed.\")\n",
        "\n",
        "#Refine prompt as per your need\n",
        "system_prompt = (\n",
        "    \"You are an expert HR and Recruitment Assistant. \"\n",
        "    \"You are analyzing a candidate's CV/Resume. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If the information is not in the CV, strictly say 'I cannot find that information in the provided CV.' \"\n",
        "    \"Keep your answers concise and professional.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "print(\"Chatbot is ready!\")"
      ],
      "metadata": {
        "id": "S4ncX2bSfMWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Start chatting with {filename} (Type 'exit' to stop):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        break\n",
        "\n",
        "    # Run the RAG chain\n",
        "    response = rag_chain.invoke({\"input\": user_input})\n",
        "\n",
        "    print(f\"AI: {response['answer']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "fqfU_XnrAqm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}